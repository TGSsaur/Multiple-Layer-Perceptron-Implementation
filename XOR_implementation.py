# -*- coding: utf-8 -*-
"""XOR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DluKNS_iagPpieNaywsW3SZJPjtFyszJ
"""

import numpy as np
from matplotlib import pyplot as plt

def sigm(z):
    return 1 / (1 + np.exp(-z))

def initialize_WB(inputF,outputF,hddL):
  Wgt1 = np.random.randn(2,2)
  print(Wgt1)
  Wgt2 = np.random.randn(2,1)
  by1 = np.zeros((hddL,1))
  by2 = np.zeros((outputF,1))

  hypPara = {"W1":Wgt1,"W2":Wgt2,"B1":by1,"B2":by2}
  return hypPara

def forwardP(input,output,para):
  w1 = para["W1"]
  w2 = para["W2"]
  b1 = para["B1"]
  b2 = para["B2"]
  print(w1)
  z1 = np.dot(input,w1) + b1
  a1 = sigm(z1)
  print(z1)
  
  print(input)
  z2 = np.dot(a1,w2) + b2
  a2 = sigm(z2)

  trackPara = (z1,a1,w1,b1,z2,a2,w2,b2)
  p = input.shape[1]
  cost_val = np.sum((output-a2)^2/p)
  return trackPara,cost_val,a2

def backwardP(input,output,trackPara):
  (z1,a1,w1,b1,z2,a2,w2,b2) = trackPara
  m = input.shape[1]
     
  dz2 = a2 - output
  dw2 = np.dot(dz2, a1.T) / m
  db2 = np.sum(dz2, axis = 1, keepdims = True)
     
  da1 = np.dot(w2.T, dz2)
  dz1 = np.multiply(da1, a1 * (1- a1))
  dw1 = np.dot(dz1, X.T) / m
  db1 = np.sum(dz1, axis = 1, keepdims = True) / m
     
  gradients = {"dZ2": dz2, "dW2": dw2, "db2": db2,
                 "dZ1": dz1, "dW1": dw1, "db1": db1}
  return gradients

def updateWeights(para,gradient,lr):
  para["W1"] = para["W1"] - gradient["dw1"]*lr
  para["W2"] = para["W2"] - gradient["dw2"]*lr
  para["B1"] = para["B1"] - gradient["db1"]*lr
  para["B2"] = para["B2"] - gradient["db1"]*lr

  return para

input = np.vstack(([0,1],[0,0],[1,1],[1,0]))
output = np.array([1,0,0,1]).reshape(-1,1)

hddL = 2 
inputF = input.shape[0] 
outputF = output.shape[0]
print(str(inputF)+" "+ str(outputF))
parameters = initialize_WB(inputF, outputF, hddL)
epoch = 100000
learningRate = 0.01
cost = np.zeros((epoch, 1))
 
for i in range(epoch):
    cache,cost[i, 0], A2 = forwardP(input,output, parameters)
    gradients = backwardP(input, output, cache)
    parameters = updateWeights(parameters, gradients, learningRate)
 
print(cost)
print(y)